{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫小tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.基本的抓站\n",
    "import urllib2\n",
    "\n",
    "content = urllib2.urlopen('http://XXXX').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.使用代理服务器\n",
    "import urllib2\n",
    "\n",
    "proxy_support = urllib2.ProxyHandler({'http':'http://XX.XX.XX.XX:XXXX'})\n",
    "\n",
    "opener = urllib2.build_opener(proxy_support, urllib2.HTTPHandler)\n",
    "\n",
    "urllib2.install_opener(opener)\n",
    "\n",
    "content = urllib2.urlopen('http://XXXX').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3.登陆\n",
    "#3.1 cookie的处理\n",
    "import urllib2, cookielib\n",
    "\n",
    "cookie_support= urllib2.HTTPCookieProcessor(cookielib.CookieJar())\n",
    "\n",
    "opener = urllib2.build_opener(cookie_support, urllib2.HTTPHandler)\n",
    "\n",
    "urllib2.install_opener(opener)\n",
    "\n",
    "content = urllib2.urlopen('http://XXXX').read()\n",
    "\n",
    "#如果想同时用代理和cookie，那就加入proxy_support然后operner改为 ：\n",
    "opener = urllib2.build_opener(proxy_support, cookie_support, urllib2.HTTPHandler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3.2 表单的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "登录必要填表，表单怎么填？首先利用工具截取所要填表的内容。\n",
    "比如我一般用firefox+httpfox插件来看看自己到底发送了些什么包\n",
    "以verycd为例，先找到自己发的POST请求，以及POST表单项\n",
    "可以看到verycd的话需要填username,password,continueURI,fk,login_submit这几项，其中fk是随机生成的（其实不太随机，看上去像是把epoch时间经过简单的编码生成的），需要从网页获取，也就是说得先访问一次网页，用正则表达式等工具截取返回数据中的fk项。continueURI顾名思义可以随便写，login_submit是固定的，这从源码可以看出。还有username，password那就很显然了。\n",
    "好的，有了要填写的数据，我们就要生成postdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "postdata=urllib.urlencode({\n",
    "\n",
    "    'username':'XXXXX',\n",
    "\n",
    "    'password':'XXXXX',\n",
    "\n",
    "    'continueURI':'http://www.verycd.com/',\n",
    "\n",
    "    'fk':fk,\n",
    "\n",
    "    'login_submit':'登录'\n",
    "\n",
    "})\n",
    "\n",
    "req = urllib2.Request(\n",
    "\n",
    "    url = 'http://secure.verycd.com/signin/*/http://www.verycd.com/',\n",
    "\n",
    "    data = postdata\n",
    "\n",
    ")\n",
    "\n",
    "result = urllib2.urlopen(req).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3.3 伪装成浏览器访问\n",
    "headers = {\n",
    "\n",
    "    'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'\n",
    "\n",
    "}\n",
    "\n",
    "req = urllib2.Request(\n",
    "\n",
    "    url = 'http://secure.verycd.com/signin/*/http://www.verycd.com/',\n",
    "\n",
    "    data = postdata,\n",
    "\n",
    "    headers = headers\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3.4 反”反盗链”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "某些站点有所谓的反盗链设置，其实说穿了很简单，就是检查你发送请求的header里面，referer站点是不是他自己，所以我们只需要像3.3一样，把headers的referer改成该网站即可，以黑幕著称地cnbeta为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "\n",
    "    'Referer':'http://www.cnbeta.com/articles'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3.5 终极绝招\n",
    "有时候即使做了3.1-3.4，访问还是会被据，那么没办法，老老实实把httpfox中看到的headers全都写上，那一般也就行了。 再不行，那就只能用终极绝招了，selenium直接控制浏览器来进行访问，只要浏览器可以做到的，那么它也可以做到。类似的还有pamie，watir，等等等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4.多线程并发抓取\n",
    "from threading import Thread\n",
    "\n",
    "from Queue import Queue\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "#q是任务队列\n",
    "\n",
    "#NUM是并发线程总数\n",
    "\n",
    "#JOBS是有多少任务\n",
    "\n",
    "q = Queue()\n",
    "\n",
    "NUM = 2\n",
    "\n",
    "JOBS = 10\n",
    "\n",
    "#具体的处理函数，负责处理单个任务\n",
    "\n",
    "def do_somthing_using(arguments):\n",
    "\n",
    "    print arguments\n",
    "\n",
    "#这个是工作进程，负责不断从队列取数据并处理\n",
    "\n",
    "def working():\n",
    "\n",
    "    while True:\n",
    "\n",
    "        arguments = q.get()\n",
    "\n",
    "        do_somthing_using(arguments)\n",
    "\n",
    "        sleep(1)\n",
    "\n",
    "        q.task_done()\n",
    "\n",
    "#fork NUM个线程等待队列\n",
    "\n",
    "for i in range(NUM):\n",
    "\n",
    "    t = Thread(target=working)\n",
    "\n",
    "    t.setDaemon(True)\n",
    "\n",
    "    t.start()\n",
    "\n",
    "#把JOBS排入队列\n",
    "\n",
    "for i in range(JOBS):\n",
    "\n",
    "    q.put(i)\n",
    "\n",
    "#等待所有JOBS完成\n",
    "\n",
    "q.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5.验证码的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.google那种验证码，凉拌\n",
    "\n",
    "2.简单的验证码：字符个数有限，只使用了简单的平移或旋转加噪音而没有扭曲的，这种还是有可能可以处理的，一般思路是旋转的转回来，噪音去掉，然后划分单个字符，划分好了以后再通过特征提取的方法(例如PCA)降维并生成特征库，然后把验证码和特征库进行比较。这个比较复杂，一篇博文是说不完的，这里就不展开了，具体做法请弄本相关教科书好好研究一下。\n",
    "\n",
    "事实上有些验证码还是很弱的，这里就不点名了，反正我通过2的方法提取过准确度非常高的验证码，所以2事实上是可行的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#6. gzip/deflate支持"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在的网页普遍支持gzip压缩，这往往可以解决大量传输时间，以VeryCD的主页为例，未压缩版本247K，压缩了以后45K，为原来的1/5。这就意味着抓取速度会快5倍。\n",
    "\n",
    "然而python的urllib/urllib2默认都不支持压缩，要返回压缩格式，必须在request的header里面写明’accept-encoding’，然后读取response后更要检查header查看是否有’content-encoding’一项来判断是否需要解码，很繁琐琐碎。如何让urllib2自动支持gzip, defalte呢？\n",
    "\n",
    "其实可以继承BaseHanlder类，然后build_opener的方式来处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "\n",
    "from gzip import GzipFile\n",
    "\n",
    "from StringIO import StringIO\n",
    "\n",
    "class ContentEncodingProcessor(urllib2.BaseHandler):\n",
    "\n",
    "  \"\"\"A handler to add gzip capabilities to urllib2 requests \"\"\"\n",
    "\n",
    " \n",
    "\n",
    "  # add headers to requests\n",
    "\n",
    "  def http_request(self, req):\n",
    "\n",
    "    req.add_header(\"Accept-Encoding\", \"gzip, deflate\")\n",
    "\n",
    "    return req\n",
    "\n",
    " \n",
    "\n",
    "  # decode\n",
    "\n",
    "  def http_response(self, req, resp):\n",
    "\n",
    "    old_resp = resp\n",
    "\n",
    "    # gzip\n",
    "\n",
    "    if resp.headers.get(\"content-encoding\") == \"gzip\":\n",
    "\n",
    "        gz = GzipFile(\n",
    "\n",
    "                    fileobj=StringIO(resp.read()),\n",
    "\n",
    "                    mode=\"r\"\n",
    "\n",
    "                  )\n",
    "\n",
    "        resp = urllib2.addinfourl(gz, old_resp.headers, old_resp.url, old_resp.code)\n",
    "\n",
    "        resp.msg = old_resp.msg\n",
    "\n",
    "    # deflate\n",
    "\n",
    "    if resp.headers.get(\"content-encoding\") == \"deflate\":\n",
    "\n",
    "        gz = StringIO( deflate(resp.read()) )\n",
    "\n",
    "        resp = urllib2.addinfourl(gz, old_resp.headers, old_resp.url, old_resp.code)  # 'class to add info() and\n",
    "\n",
    "        resp.msg = old_resp.msg\n",
    "\n",
    "    return resp\n",
    "\n",
    " \n",
    "\n",
    "# deflate support\n",
    "\n",
    "import zlib\n",
    "\n",
    "def deflate(data):   # zlib only provides the zlib compress format, not the deflate format;\n",
    "\n",
    "  try:               # so on top of all there's this workaround:\n",
    "\n",
    "    return zlib.decompress(data, -zlib.MAX_WBITS)\n",
    "\n",
    "  except zlib.error:\n",
    "\n",
    "    return zlib.decompress(data)\n",
    "\n",
    "\n",
    "\n",
    "然后就简单了，\n",
    "\n",
    "\n",
    "\n",
    "encoding_support = ContentEncodingProcessor\n",
    "\n",
    "opener = urllib2.build_opener( encoding_support, urllib2.HTTPHandler )\n",
    "\n",
    " \n",
    "\n",
    "#直接用opener打开网页，如果服务器支持gzip/defalte则自动解压缩\n",
    "\n",
    "content = opener.open(url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#7. 更方便地多线程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#7.1.用twisted进行异步I/O抓取\n",
    "from twisted.web.client import getPage\n",
    "\n",
    "from twisted.internet import reactor\n",
    "\n",
    " \n",
    "\n",
    "links = [ 'http://www.verycd.com/topics/%d/'%i for i in range(5420,5430) ]\n",
    "\n",
    " \n",
    "\n",
    "def parse_page(data,url):\n",
    "\n",
    "    print len(data),url\n",
    "\n",
    " \n",
    "\n",
    "def fetch_error(error,url):\n",
    "\n",
    "    print error.getErrorMessage(),url\n",
    "\n",
    " \n",
    "\n",
    "# 批量抓取链接\n",
    "\n",
    "for url in links:\n",
    "\n",
    "    getPage(url,timeout=5) \\\n",
    "\n",
    "        .addCallback(parse_page,url) \\ #成功则调用parse_page方法\n",
    "\n",
    "        .addErrback(fetch_error,url)     #失败则调用fetch_error方法\n",
    "\n",
    " \n",
    "\n",
    "reactor.callLater(5, reactor.stop) #5秒钟后通知reactor结束程序\n",
    "\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#7.2 设计一个简单的多线程抓取类\n",
    "Fetcher类:\n",
    "f = Fetcher(threads=10) #设定下载线程数为10\n",
    "\n",
    "for url in urls:\n",
    "\n",
    "    f.push(url)  #把所有url推入下载队列\n",
    "\n",
    "while f.taskleft(): #若还有未完成下载的线程\n",
    "\n",
    "    content = f.pop()  #从下载完成队列中取出结果\n",
    "\n",
    "    do_with(content) # 处理content内容\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "\n",
    "from threading import Thread,Lock\n",
    "\n",
    "from Queue import Queue\n",
    "\n",
    "import time\n",
    "\n",
    " \n",
    "\n",
    "class Fetcher:\n",
    "\n",
    "    def __init__(self,threads):\n",
    "\n",
    "        self.opener = urllib2.build_opener(urllib2.HTTPHandler)\n",
    "\n",
    "        self.lock = Lock() #线程锁\n",
    "\n",
    "        self.q_req = Queue() #任务队列\n",
    "\n",
    "        self.q_ans = Queue() #完成队列\n",
    "\n",
    "        self.threads = threads\n",
    "\n",
    "        for i in range(threads):\n",
    "\n",
    "            t = Thread(target=self.threadget)\n",
    "\n",
    "            t.setDaemon(True)\n",
    "\n",
    "            t.start()\n",
    "\n",
    "        self.running = 0\n",
    "\n",
    " \n",
    "\n",
    "    def __del__(self): #解构时需等待两个队列完成\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        self.q_req.join()\n",
    "\n",
    "        self.q_ans.join()\n",
    "\n",
    " \n",
    "\n",
    "    def taskleft(self):\n",
    "\n",
    "        return self.q_req.qsize()+self.q_ans.qsize()+self.running\n",
    "\n",
    " \n",
    "\n",
    "    def push(self,req):\n",
    "\n",
    "        self.q_req.put(req)\n",
    "\n",
    " \n",
    "\n",
    "    def pop(self):\n",
    "\n",
    "        return self.q_ans.get()\n",
    "\n",
    " \n",
    "\n",
    "    def threadget(self):\n",
    "\n",
    "        while True:\n",
    "\n",
    "            req = self.q_req.get()\n",
    "\n",
    "            with self.lock: #要保证该操作的原子性，进入critical area\n",
    "\n",
    "                self.running += 1\n",
    "\n",
    "            try:\n",
    "\n",
    "                ans = self.opener.open(req).read()\n",
    "\n",
    "            except Exception, what:\n",
    "\n",
    "                ans = ''\n",
    "\n",
    "                print what\n",
    "\n",
    "            self.q_ans.put((req,ans))\n",
    "\n",
    "            with self.lock:\n",
    "\n",
    "                self.running -= 1\n",
    "\n",
    "            self.q_req.task_done()\n",
    "\n",
    "            time.sleep(0.1) # don't spam\n",
    "\n",
    " \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    links = [ 'http://www.verycd.com/topics/%d/'%i for i in range(5420,5430) ]\n",
    "\n",
    "    f = Fetcher(threads=10)\n",
    "\n",
    "    for url in links:\n",
    "\n",
    "        f.push(url)\n",
    "\n",
    "    while f.taskleft():\n",
    "\n",
    "        url,content = f.pop()\n",
    "\n",
    "        print url,len(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#8.其他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "连接池：\n",
    "opener.open和urllib2.urlopen一样，都会新建一个http请求。通常情况下这不是什么问题，因为线性环境下，一秒钟可能也就新生成一个请求；然而在多线程环境下，每秒钟可以是几十上百个请求，这么干只要几分钟，正常的有理智的服务器一定会封禁你的。\n",
    "\n",
    "然而在正常的html请求时，保持同时和服务器几十个连接又是很正常的一件事，所以完全可以手动维护一个HttpConnection的池，然后每次抓取时从连接池里面选连接进行连接即可。\n",
    "\n",
    "这里有一个取巧的方法，就是利用squid做代理服务器来进行抓取，则squid会自动为你维护连接池，还附带数据缓存功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设定线程的栈大小：\n",
    "栈大小的设定将非常显著地影响python的内存占用，python多线程不设置这个值会导致程序占用大量内存，这对openvz的vps来说非常致命。stack_size必须大于32768，实际上应该总要32768*2以上\n",
    "\n",
    "from threading import stack_size\n",
    "\n",
    "stack_size(32768*16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置失败后自动重试:\n",
    "    def get(self,req,retries=3):\n",
    "\n",
    "        try:\n",
    "\n",
    "            response = self.opener.open(req)\n",
    "\n",
    "            data = response.read()\n",
    "\n",
    "        except Exception , what:\n",
    "\n",
    "            print what,req\n",
    "\n",
    "            if retries&gt;0:\n",
    "\n",
    "                return self.get(req,retries-1)\n",
    "\n",
    "            else:\n",
    "\n",
    "                print 'GET Failed',req\n",
    "\n",
    "                return ''\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置超时:\n",
    "\n",
    "import socket\n",
    "\n",
    "socket.setdefaulttimeout(10) #设置10秒后连接超时\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "登陆:\n",
    "    \n",
    "登陆更加简化了，首先build_opener中要加入cookie支持，参考“总结”一文；如要登陆VeryCD，给Fetcher新增一个空方法login，并在init()中调用，然后继承Fetcher类并override login方法\n",
    "def login(self,username,password):\n",
    "\n",
    "    import urllib\n",
    "\n",
    "    data=urllib.urlencode({'username':username,\n",
    "\n",
    "                           'password':password,\n",
    "\n",
    "                           'continue':'http://www.verycd.com/',\n",
    "\n",
    "                           'login_submit':u'登录'.encode('utf-8'),\n",
    "\n",
    "                           'save_cookie':1,})\n",
    "\n",
    "    url = 'http://www.verycd.com/signin'\n",
    "\n",
    "    self.opener.open(url,data).read()\n",
    "\n",
    "\n",
    "\n",
    "于是在Fetcher初始化时便会自动登录VeryCD网站"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
